---
layout:     post
title:      Tensorflow 2.0 04
subtitle:   RNN In One #å‰¯æ ‡é¢˜
date:       2019-11-02
author:     ASeeker
header-img: img/BetterDays1.jpg
catalog: true
tags:
    - tensorflow
    - deep learning
    
---

#### ä½ å¾€å‰èµ°ï¼Œæˆ‘ä¸€å®šåœ¨ä½ åé¢ã€‚

å¾ˆæ£’çš„ä¸€éƒ¨ç”µå½±ï¼Œåæ˜ äº†ä¸å°‘ä¸­å›½çš„ç°çŠ¶ï¼Œå‘¨å†¬é›¨å’Œåƒçºçš„æ¼”æŠ€ä¹Ÿå¾ˆä¸é”™ï¼Œè™½ç„¶è¿˜æœ‰äº›ç¼ºæ†¾ï¼Œå¿ƒç›®ä¸­ä»Šå¹´çš„æœ€ä½³åè¯­å½±ç‰‡ğŸ‘‘

RNNç³»åˆ—  
é¡ºå¸¦å¤ä¹ ä¸€ä¸‹RNN  

> Reference[^1] [^2] [^3] [^4] [^5] [^6] [^7] [^8] [^9] [^10]

#### RNN --> LSTM

è¿™é‡Œç›´æ¥å†™LSTMï¼Œæœ€ç®€å•çš„RNNä¼šå‡ºç°vanishçš„é—®é¢˜  
ä»£ç ä¸­ä¹Ÿç›´æ¥å°±ç”¨`LSTM`äº†ï¼Œè¿˜æœ‰ä¸€ç§`LSTMCell`,éœ€è¦è‡ªå·±æ¯æ¬¡forå¾ªç¯timestep  

ä¹‹å‰æ˜¯åœ¨ngå’Œæå®æ¯…çš„è§†é¢‘é‡Œçœ‹çš„RNNï¼Œä¸è¿‡æ€»æ„Ÿè§‰è§†é¢‘è®²çš„æœ‰ç‚¹æµ…ï¼Œåœ¨[è¿™ç¯‡åšå®¢](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)é‡Œé¢è®²çš„å¾ˆå…·ä½“ã€‚è¿™äº›dalaoçš„åšå®¢å†™çš„çœŸæ˜¯å¤ªå¥½äº†ï¼Œå¸Œæœ›è‡ªå·±ä»¥åä¹Ÿå¯ä»¥å†™å‡ºè¿™æ ·æœ‰ä»·å€¼çš„æ–‡ç« åˆ†äº«ç»™ä»–äººğŸ˜Šã€‚   

å¾ˆç®€å•çš„è§£é‡Šäº†ä¸€ä¸‹rnnçš„ä¸è¶³  
 
>
If we are trying to predict the last word in `â€œthe clouds are in the sky,â€` we donâ€™t need any further context â€“ itâ€™s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that itâ€™s needed is small, RNNs can learn to use the past information.
    
>Consider trying to predict the last word in the text `â€œI grew up in Franceâ€¦ I speak fluent French.â€` Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. Itâ€™s entirely possible for the gap between the relevant information and the point where it is needed to become very large.  

å¯¹äºLSTMçš„ä¸€ä¸ªintuitiveçš„ç†è§£

>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.  
The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. Itâ€™s very easy for information to just flow along it unchanged.

æœ€åè¿˜æœ‰ä¸€äº›LSTMçš„variations


#### å…³äºBPTT  
ä»¥å‰æ²¡æœ‰ä»”ç»†æ¨å¯¼è¿‡bpttï¼Œåªæ˜¯ä»ç›´è§‚ä¸Šå»äº†è§£ï¼Œè¿™é‡Œçœ‹äº†ä¸‹[è¿™ç¯‡åšå®¢](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/),è§£é‡Šçš„è¿˜æŒºä¸é”™çš„ã€‚BPTTå…¶å®å°±æ˜¯å¯¹æ¯ä¸ªtimestepåšsumã€‚ä¹Ÿä»å…¬å¼æ¨å¯¼ä¸Šå¯¹vanishé—®é¢˜æœ‰äº†è¿›ä¸€æ­¥çš„äº†è§£ã€‚è¿™ä¸ª[github](ä¸Šé¢)æœ‰æ›´è¯¦ç»†çš„æ¨å¯¼ã€‚        


è¿™è¾¹å‰é¢éƒ½æ˜¯ä¸å‰é¢ä¸€æ ·çš„å°±ä¸å¤šè¯´äº†ï¼Œè¿™é‡Œè¿˜æ˜¯ç”¨mnistï¼Œæ¯ä¸€è¡Œçœ‹ä½œä¸€ä¸ªtimestep      

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist
```


```python
batch_size = 32
train_steps = 1000
display_step = 100

lstm_units = 32
num_classes = 10

learning_rate = 1e-3

```


```python
(x_train, y_train), _ = mnist.load_data()

x_train = x_train.astype(np.float32).reshape([-1,28,28]) / 255.

train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train))
train_data = train_data.repeat().shuffle(10000).batch(batch_size).prefetch(1)
```

å¾ˆæ–¹ä¾¿ï¼Œç›´æ¥ç»™ä¸€ä¸ªLSTMå±‚å°±å¯ä»¥äº†  

```python
class Lstm(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.lstm1 = tf.keras.layers.LSTM(lstm_units)
        self.out = tf.keras.layers.Dense(num_classes)
    
    def call(self, inputs, is_training=False):
        outputs = self.lstm1(inputs)
        outputs = self.out(outputs)
        if not is_training:
            outputs = tf.nn.softmax(outputs)
        
        return outputs

lstm = Lstm()
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

def acc(y_pred, y_true):
    y_true = tf.cast(y_true, tf.int64)
    acc_bool = tf.equal(tf.argmax(y_pred, axis=-1), y_true)
    acc_rate = tf.reduce_mean(tf.cast(acc_bool, tf.float32))
    
    return acc_rate
```

å¼€å§‹è®­ç»ƒ  

```python
for step, (batch_x, batch_y) in enumerate(train_data.take(train_steps+1)):
    with tf.GradientTape() as tape:
        y_pred = lstm(batch_x, is_training=True)
        batch_y = tf.cast(batch_y, tf.int32)
        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_pred, labels=batch_y))
    
    grads = tape.gradient(loss, lstm.variables)
    optimizer.apply_gradients(grads_and_vars=zip(grads, lstm.variables))
    
    if step % display_step == 0:
        print('step %d: loss %f  acc %f' %(step, loss, acc(y_pred, batch_y)))



```

step 0: loss 2.307400  acc 0.093750  
step 100: loss 1.696196  acc 0.406250  
step 200: loss 1.099894  acc 0.625000  
step 300: loss 1.079787  acc 0.593750  
step 400: loss 0.856504  acc 0.718750  
step 500: loss 0.447243  acc 0.875000  
step 600: loss 0.541385  acc 0.812500  
step 700: loss 0.699608  acc 0.781250  
step 800: loss 0.417174  acc 0.875000  
step 900: loss 0.431013  acc 0.812500  
step 1000: loss 0.365609  acc 0.875000  


æµ‹è¯•ä¸€ä¸‹ lstmç”¨åœ¨mnistä¸Šçš„æ•ˆæœè¿˜æ˜¯å¾ˆä¸€èˆ¬çš„  

```python
x_test,y_test = _

x_test = x_test.astype(np.float32) / 255.

y_test_pred = lstm(x_test, is_training=False)

print('test acc: %f' %(acc(y_test_pred, y_test)))
```

   test acc: 0.860400


å…³äºtfå®ç°çš„LSTMçš„è¾“å‡ºæ˜¯1ã€2ã€3ä¸ªï¼Œå¯ä»¥å‚è€ƒ[è¿™ç¯‡åšå®¢](https://huhuhang.com/post/machine-learning/lstm-return-sequences-state)


#### Dynamic Rnn

ä¸€å¼€å§‹ä¸çŸ¥é“å•¥æ˜¯dynamicï¼Œåæ¥ä¸€æŸ¥æ˜¯ä¸ºäº†è§£å†³timestepç¼ºå¤±æˆ–è€…æ•°æ®é›†é•¿çŸ­ä¸ä¸€çš„æƒ…å†µçš„   
å¤§ä½“æ­¥éª¤æ˜¯å…ˆå°†æ¯æ¡æ•°æ®paddingæˆmaxçš„timestep  
ç„¶åç¼ºå¤±çš„timestepç»™ç‰¹å®šçš„maskå€¼  
åœ¨æ­å»ºæ¨¡å‹çš„æ—¶å€™åŠ å…¥ä¸€ä¸ª`Masking`å±‚  
åé¢å°±åˆéƒ½æ˜¯ä¸€æ ·çš„äº†  

å…·ä½“å¯ä»¥çœ‹[è¿™å„¿](https://www.tensorflow.org/guide/keras/masking_and_padding)




```python
import tensorflow as tf
import numpy as np

batch_size = 32

lstm_units = 32
classes_num = 2

train_steps = 2000
display_step = 200
learning_rate = 1e-3

masking_value = -1

seq_min_length = 5
seq_max_length = 20
max_value = 10000

```

è®­ç»ƒæ•°æ®çš„ç”Ÿæˆ   
å‡è®¾è¿ç»­çš„æ•°æ® labelä¸º0   
ä¸è¿ç»­çš„labelä¸º1  
è¿™é‡Œdatasetçš„å†™æ³•ä½¿ç”¨äº†from_generator

```python
def toy_data_generator():
    
    while True:
        
        seq_length = np.random.randint(seq_min_length, seq_max_length)
        rand_start = np.random.randint(0, max_value-seq_length)
    
        if np.random.random() > 0.5:
            seq = np.arange(rand_start, rand_start+seq_length)
            seq = seq / max_value
            
            seq = np.pad(seq, mode='constant', pad_width=(0, seq_max_length-seq_length), constant_values=masking_value)
            
            label = 0
        else:
            seq = np.random.randint(0, max_value, size=seq_length)
            seq = seq / max_value
            seq = np.pad(seq, mode='constant', pad_width=(0, seq_max_length-seq_length), constant_values=masking_value)
            label = 1
        
        yield np.array(seq, dtype=np.float32), np.array(label, dtype=np.int64)

train_data = tf.data.Dataset.from_generator(toy_data_generator, output_types=(tf.float32, tf.int64))
train_data = train_data.repeat().shuffle(1000).batch(batch_size).prefetch(1)
```


```python
class DynamicRNN(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.masking1 = tf.keras.layers.Masking(mask_value=masking_value)
        self.lstm1 = tf.keras.layers.LSTM(units=lstm_units)
        self.out = tf.keras.layers.Dense(classes_num)
        
    def call(self, inputs, is_training=False):
        # A RNN Layer expects a 3-dim input (batch_size, seq_len, num_features).
        outputs = tf.expand_dims(inputs, -1)
        outputs = self.masking1(outputs)
        outputs = self.lstm1(outputs)
        outputs = self.out(outputs)
        
        if not is_training:
            outputs = tf.nn.softmax(outputs)
            
        return outputs
```

è®­ç»ƒ  

```python
dyrnn = DynamicRNN()
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

def acc(y_pred, y_true):
    return tf.reduce_mean( tf.cast( tf.equal( tf.argmax(y_pred, axis=-1), y_true)  ,tf.float32 )  )

for step, (batch_x, batch_y) in enumerate(train_data.take(train_steps+1)):
    with tf.GradientTape() as tape:
        
        y_pred = dyrnn(batch_x, is_training=True)
        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_pred, labels=batch_y))
    grads = tape.gradient(loss, dyrnn.variables)
    optimizer.apply_gradients(zip(grads, dyrnn.variables))

    if step % display_step == 0:
        print('step %d  loss: %f  acc: %f' % (step, loss, acc(y_pred, batch_y)))
    
```

step 0  loss: 0.720475  acc: 0.437500   
step 200  loss: 0.578635  acc: 0.750000  
step 400  loss: 0.463771  acc: 0.750000  
step 600  loss: 0.461265  acc: 0.750000  
step 800  loss: 0.454481  acc: 0.875000  
step 1000  loss: 0.279865  acc: 0.875000  
step 1200  loss: 0.221204  acc: 0.937500  
step 1400  loss: 0.135849  acc: 0.937500  
step 1600  loss: 0.138664  acc: 1.000000  
step 1800  loss: 0.234895  acc: 0.906250  
step 2000  loss: 0.180870  acc: 1.000000  


#### Bidirectional Rnn

æœ€åç™»åœºçš„æ˜¯æˆ‘ä»¬çš„åŒå‘RNNğŸŒŸ  
tfï¼ˆkerasï¼‰é‡Œé¢ä¹Ÿå¸®æˆ‘ä»¬å®ç°çš„å¾ˆå¥½äº†ï¼Œè°ƒä¸€ä¸ª`Bidirectional`å°±å¥½äº†ï¼Œå¦‚æœä½ æƒ³è¦ä¸¤ä¸ªæ–¹å‘ä¸Šçš„ç½‘ç»œé•¿å¾—ä¸€æ ·ï¼Œå°±ç›´æ¥ä¼ ä¸€ä¸ªå°±å¥½äº†ï¼Œtfä¼šè‡ªå·±å¤åˆ¶ä¸€ä¸‹ï¼›ï¼ˆæƒ³ä¸ä¸€æ ·å°±ä¼ ä¸¤ä¸ªå¥½äº†  [docè§æ­¤](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)


```python
import tensorflow as tf
import numpy as np

batch_size = 32
learning_rate = 1e-3
train_steps = 2000
display_step = 200

classes_num = 10
lstm1_units = 32

from tensorflow.keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.

y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)

train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_data = train_data.repeat().shuffle(10000).batch(batch_size).prefetch(1)
```

åå‘çš„é‚£ä¸ªè®°å¾—è¦`go_backwards`  

```python
class BiLSTM(tf.keras.Model):
    def __init__(self):
        super().__init__()
        lstm_fw1 = tf.keras.layers.LSTM(units=lstm1_units)
        lstm_bw1 = tf.keras.layers.LSTM(units=lstm1_units, go_backwards=True)
        self.bilstm1 = tf.keras.layers.Bidirectional(lstm_fw1, backward_layer=lstm_bw1)
        self.out = tf.keras.layers.Dense(units=classes_num)
        
    def call(self, inputs, is_training=False):
        outputs = self.bilstm1(inputs)
        outputs = self.out(outputs)
        
        if not is_training:
            outputs = tf.nn.softmax(outputs)
            
        return outputs    


biLstm = BiLSTM()
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)


def acc(y_pred, y_true):
    
    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_pred, axis=-1) ,y_true) ,tf.float32))
    

for step, (batch_x, batch_y) in enumerate(train_data.take(train_steps+1)):
    with tf.GradientTape() as tape:
        y_pred = biLstm(batch_x)
        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_pred, labels=batch_y))
    grads = tape.gradient(loss, biLstm.variables)
    optimizer.apply_gradients(grads_and_vars=zip(grads, biLstm.variables))
    
    if step % display_step == 0:
        print('step %d loss: %f  acc: %f'%(step, loss, acc(y_pred, batch_y)))
    
```

step 0 loss: 2.303555  acc: 0.062500  
step 200 loss: 1.889396  acc: 0.593750  
step 400 loss: 1.818122  acc: 0.687500  
step 600 loss: 1.729275  acc: 0.750000  
step 800 loss: 1.540786  acc: 0.968750  
step 1000 loss: 1.793411  acc: 0.687500  
step 1200 loss: 1.596297  acc: 0.875000  
step 1400 loss: 1.683933  acc: 0.781250  
step 1600 loss: 1.580734  acc: 0.875000  
step 1800 loss: 1.685784  acc: 0.750000  
step 2000 loss: 1.616963  acc: 0.875000  


biçš„æ•ˆæœç¡®å®è¦å¥½äº†ä¸€ç‚¹  

```python
y_test_pred = biLstm(x_test)
print('test acc %f' %(acc(y_test_pred, y_test)))
```

   test acc 0.895500







#### è¿˜æœ‰ä¸€äº›

[è¿™é‡Œ](https://www.tensorflow.org/guide/keras/rnn)æœ‰ä¸€ä¸ªrnn tfçš„å®˜æ–¹guide  

[tf.tile()å‡½æ•°](https://blog.csdn.net/xwd18280820053/article/details/72867818)  

[np.pad()å‡½æ•°](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html)


å¥½äº†ï¼Œå…³äºRNNç³»åˆ—æœ‰äº†ä¸€ä¸ªæ¯”è¾ƒç³»ç»Ÿçš„å­¦ä¹ ã€‚  
ï¼ˆä¸è¿‡è¿™ç¯‡åšå®¢å®åœ¨æ˜¯æ‹–äº†å¤ªä¹…äº†ï¼Œï¼Œï¼ŒçœŸçš„ä¸åº”è¯¥

æœ€è¿‘çœ‹åˆ°äº†å…³äºè±«ç« ä¹¦é™¢çš„æ–°é—»ã€‚ã€‚ä¸€å¼€å§‹è‡ªå·±ä¹‰æ„¤å¡«è†ºï¼Œçœ‹äº†å¾ˆå¤šæ–‡ç« ã€upä¸»çš„è§†é¢‘ï¼Œæœ‰å„ç§å„æ ·çš„å£°éŸ³ã€‚åæ¥å†·é™ä¸‹æ¥æƒ³æƒ³ï¼Œè¿™ç§äº‹æƒ…å¯¹äºæ”¿åºœæ¥è¯´ç¡®å®å¾ˆéš¾å¤„ç†ã€‚ç›¸ä¿¡æˆ‘ä»¬çš„å›½å®¶ä¼šè¶Šæ¥è¶Šå¥½çš„ã€‚

----

[^1]: http://colah.github.io/posts/2015-08-Understanding-LSTMs/

[^2]: http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/

[^3]: https://github.com/go2carter/nn-learn/blob/master/grad-deriv-tex/rnn-grad-deriv.pdf

[^4]: https://www.tensorflow.org/guide/keras/rnn

[^5]: https://www.tensorflow.org/guide/keras/masking_and_padding

[^6]: https://huhuhang.com/post/machine-learning/lstm-return-sequences-state

[^7]: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional

[^8]: https://blog.csdn.net/xwd18280820053/article/details/72867818

[^9]: https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html

[^10]: https://github.com/aymericdamien/TensorFlow-Examples/tree/master/tensorflow_v2/notebooks/3_NeuralNetworks