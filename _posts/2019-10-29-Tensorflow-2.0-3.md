---
layout:     post
title:      Tensorflow 2.0 03
subtitle:   Simple Neural Network & Convolutional Network #å‰¯æ ‡é¢˜
date:       2019-10-29
author:     ASeeker
header-img: img/shiyuan3.jpg
catalog: true
tags:
    - tensorflow
    - deep learning
    
---

#### æœ‰å·¥å¤«ç»æœ›çš„è¯ï¼Œè¿˜ä¸å¦‚åƒç‚¹å¥½åƒçš„å»ç¡è§‰å‘¢ã€‚


ä»Šå¤©æœ‰ç‚¹åˆ’æ°´äº†ã€‚ã€‚æ˜¨æ™šå†™å½±è¯„å†™çš„å¤ªæ™šï¼Œä»Šå¤©ä¸Šåˆç›´æ¥å°±æ²¡äº†ã€‚ã€‚ã€‚å¥½ä¹…æ²¡é€›Bç«™ æ™šä¸Šåˆå •è½çš„çœ‹äº†å¥½ä¹…ã€‚ã€‚ã€‚

æ‰€ä»¥å°±åªå®ç°ä¸€ä¸‹æœ€ç®€å•çš„nnä»¥åŠcnnï¼Œstrengthenä¸€ä¸‹tfçš„ç†Ÿç»ƒåº¦     

> Reference[^1] [^2] [^3]

#### Simple Neural Network

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist
```

ä¸€äº›æ¨¡å‹å‚æ•°

```python
num_features = 784
num_h1 = 1024
num_h2 = 512
num_classes = 10

learning_rate = 1e-1
batch_size = 256
train_steps = 2000
display_per_step = 100
```
ä¸€å¦‚æ—¢å¾€çš„é€‰æ‹©mnistæ•°æ®é›†  
è¿™é‡Œç”±äºç›´æ¥ç”¨ä¸€ä¸ªdenseçš„ç½‘ç»œï¼Œå°†è¾“å…¥æ•°æ®æ‹‰å¹³åˆ°784  

```python
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.astype(np.float32).reshape([-1,num_features]) / 255.
x_test = x_test.astype(np.float32).reshape([-1,num_features]) / 255.

train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_data = train_data.shuffle(1000).repeat().batch(batch_size).prefetch(1)
```

æ­å»ºæ¨¡å‹

```python
class NeuralNet(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(units=num_h1, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(units=num_h2, activation=tf.nn.relu)
        self.out = tf.keras.layers.Dense(units=num_classes)
            
    def call(self, inputs, is_train=False):
        outputs = self.dense1(inputs)
        outputs = self.dense2(outputs)
        outputs = self.out(outputs)
        if not is_train:
            outputs = tf.nn.softmax(outputs)
        
        return outputs
        
        
nn = NeuralNet()
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
```
å®šä¹‰ä¸€ä¸‹acc metric

```python
def acc(y_pred, y_true):
    y_true = tf.cast(y_true, tf.int64)
    acc_bool = tf.equal(tf.argmax(y_pred,axis=-1), y_true)
    acc_rate = tf.reduce_mean(tf.cast(acc_bool, tf.float32))
    
    return acc_rate
```
å¼€å§‹è®­ç»ƒ

```python
for step, (batch_x, batch_y) in enumerate(train_data.take(train_steps+1)):
    with tf.GradientTape() as tape:
        y_logits = nn(batch_x, True)
        batch_y = tf.cast(batch_y, tf.int32)
        loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(labels=batch_y, logits=y_logits) )
        
    grads = tape.gradient(loss, nn.variables)
    optimizer.apply_gradients(grads_and_vars=zip(grads, nn.variables))
    
    if step % display_per_step == 0:
        acc_rate = acc(y_logits, batch_y)
        print("step: %i, loss: %f, accuracy: %f" % (step, loss, acc_rate))
    
```

 step: 0, loss: 2.302607, accuracy: 0.070312  
 step: 100, loss: 0.382137, accuracy: 0.886719  
 step: 200, loss: 0.312384, accuracy: 0.890625  
 step: 300, loss: 0.336042, accuracy: 0.898438  
 step: 400, loss: 0.252011, accuracy: 0.925781  
 step: 500, loss: 0.221575, accuracy: 0.937500  
 step: 600, loss: 0.143101, accuracy: 0.964844  
 step: 700, loss: 0.107586, accuracy: 0.968750  
 step: 800, loss: 0.112678, accuracy: 0.968750  
 step: 900, loss: 0.151181, accuracy: 0.953125  
 step: 1000, loss: 0.102401, accuracy: 0.972656  
 step: 1100, loss: 0.134542, accuracy: 0.960938  
 step: 1200, loss: 0.099323, accuracy: 0.980469  
 step: 1300, loss: 0.094068, accuracy: 0.976562  
 step: 1400, loss: 0.066545, accuracy: 0.976562  
 step: 1500, loss: 0.124233, accuracy: 0.968750  
 step: 1600, loss: 0.120372, accuracy: 0.960938  
 step: 1700, loss: 0.119279, accuracy: 0.968750  
 step: 1800, loss: 0.111487, accuracy: 0.976562  
 step: 1900, loss: 0.068802, accuracy: 0.972656  
 step: 2000, loss: 0.082421, accuracy: 0.964844  


æµ‹è¯•é›†ä¸Šçš„acc

```python
test_pred = nn(x_test, False)
print('test acc: %f' %(acc(test_pred, y_test)))

```

  test acc: 0.972100

çœ‹ä¸€ä¸‹å®é™…çš„æ•ˆæœ  

```python
import matplotlib.pyplot as plt

n_tests = 4
to_test_img = x_test[:n_tests]
pred_test = nn.predict(to_test_img)

for i in range(n_tests):
    plt.imshow(to_test_img[i].reshape([28,28]), cmap='gray')
    plt.show()
    print('pred label: %i' %(tf.argmax(pred_test[i]).numpy()))

```


![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8fgs6tlnsj306z06w742.jpg)


`pred label: 7`



![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8fgsls80cj306z06wa9u.jpg)


`pred label: 2`



![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8fgsvs87qj306z06w3ya.jpg)


`pred label: 1`



![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8fgt8vt7yj306z06wa9u.jpg)


`pred label: 0`

è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªrawçš„å†™æ³•ï¼Œæ‰€æœ‰çš„å˜é‡éƒ½è‡ªå·±å®šä¹‰ ç„¶å`tf.matmul`ã€ `tf.add`æ‰‹åŠ¨è®¡ç®—   
å¤§éƒ¨åˆ†ä»£ç éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå°±æ²¡æœ‰å†å†™ä¸€éäº† ä¸è¿‡æœ‰ä¸€ä¸ªcross_entropyçš„æ‰‹åŠ¨è®¡ç®—ï¼Œè¿˜æ˜¯æœ‰å¿…è¦çœ‹ä¸€ä¸‹çš„  
é‡Œé¢æœ‰ä¸€ä¸ª`tf.clip_by_value`å‡½æ•°   
tf.clip_by_value(A, min, max)ï¼šè¾“å…¥ä¸€ä¸ªå¼ é‡Aï¼ŒæŠŠAä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ çš„å€¼éƒ½å‹ç¼©åœ¨minå’Œmaxä¹‹é—´ã€‚å°äºminçš„è®©å®ƒç­‰äºminï¼Œå¤§äºmaxçš„å…ƒç´ çš„å€¼ç­‰äºmax    
è¿™æ˜¯é¿å…logå‡½æ•°æ¥è¿‘é›¶æ—¶ä¼šå‡ºç°è´Ÿæ— ç©·çš„æƒ…å†µ è€Œtfå°è£…å¥½çš„losså·²ç»å¸®æˆ‘ä»¬å¤„ç†è¿‡äº†  

```python
def cross_entropy(y_pred, y_true):
    # Encode label to a one hot vector.
    y_true = tf.one_hot(y_true, depth=num_classes)
    # Clip prediction values to avoid log(0) error.
    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)
    # Compute cross-entropy.
    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))
```




bugæ€»ç»“
å†™ç€éƒ¨åˆ†ä»£ç çš„æ—¶å€™ç¬”è€…é‡åˆ°äº†ä¸¤ä¸ªbugï¼Œä¸€ä¸ªæ˜¯
`acc_rate = tf.reduce_mean(tf.cast(acc_bool, tf.float32))`
è¿™è¡Œcodeæ—¶ä¸€å¼€å§‹castæˆäº†tf.int32,æ²¡æ³¨æ„åˆ°æ˜¯reduce_meanï¼Œå¯¼è‡´äº†accç®—å‡ºæ¥éƒ½æ˜¯0
è¿˜æœ‰ä¸€ä¸ªæ³¨æ„mnistå¯¼å…¥çš„è¿™ä¸ªlabelçš„æ•°æ®ç±»å‹æ˜¯unit8ï¼Œé€åˆ°lossé‡Œé¢çš„æ—¶å€™ç›´æ¥æŠ¥é”™äº†
`labels=tf.cast(batch_y, tf.int64)`   
å¯¹äº†ï¼Œè¿™é‡Œå†æä¸€ä¸‹tfçš„losså‡½æ•°çœŸçš„æ˜¯å¤ªå¤šäº†ã€‚ã€‚ã€‚ã€‚æœ‰æœºä¼šä¸€å®šè¦æ€»ç»“ä¸€ä¸‹  
è¿™ä¸ª`tf.nn.sparse_softmax_cross_entropy_with_logits`å‡½æ•°çš„å®˜æ–¹docè¿™æ ·æè¿°çš„ï¼Œlabelså¿…é¡»è¦intç±»å‹   
A common use case is to have logits of shape [batch_size, num_classes] and have labels of shape [batch_size], but higher dimensions are supported, in which case the dim-th dimension is assumed to be of size num_classes. logits must have the dtype of float16, float32, or float64, and labels must have the dtype of int32 or int64.  


#### Simple Convolutional Network


å¸¸è§„æ“ä½œ  

```python
import tensorflow as tf
import numpy as np
from tensorflow.keras.datasets import mnist

batch_size = 32
learning_rate = 1e-3
train_steps = 2000
display_step = 100

num_classes = 10

conv1_filters = 32
conv2_filters = 64
dense1_units = 1024
dropout1_rate = 0.5

```

è¿™éƒ¨åˆ†è¦æ³¨æ„ä¸€ä¸‹é€åˆ°cnnè¦reshapeæˆ4d  
grayå›¾åƒå°±åŠ ä¸€ä¸ªchannel  

```python
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.astype(np.float32).reshape([-1,28,28,1]) / 255.
x_test = x_test.astype(np.float32) / 255.

train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_data = train_data.repeat().shuffle(10000).batch(batch_size).prefetch(1)
```

è¿™éƒ¨åˆ†æ­å»ºä¸€ä¸‹ç½‘ç»œ  
å‚æ•°å˜largeäº†ï¼Œæ·»åŠ ä¸€ä¸ªdropouté˜²æ­¢overfitting  soè¿™æ˜¯å°±è¦å¼•å…¥`is_training`è¿™ä¸ªå‚æ•°äº†ï¼Œå¦‚æœæ˜¯testçš„æ—¶å€™å°±ä¸ç”¨dropoutäº†  

```python
class ConvNet(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.conv1 = tf.keras.layers.Conv2D(filters=conv1_filters,
                                            kernel_size=5,
                                            padding='same',
                                            activation=tf.nn.relu)
        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=2,
                                               strides=2)
        self.conv2 = tf.keras.layers.Conv2D(filters=conv2_filters,
                                            kernel_size=3,
                                            padding='same',
                                            activation=tf.nn.relu)
        self.pool2 = tf.keras.layers.MaxPooling2D(pool_size=2,
                                                  strides=2)
        
        self.flatten = tf.keras.layers.Flatten()
        
        self.dense1 = tf.keras.layers.Dense(units=dense1_units,
                                            activation=tf.nn.relu)
        
        self.dropout1 = tf.keras.layers.Dropout(rate=dropout1_rate)
        
        self.out = tf.keras.layers.Dense(units=num_classes)
    
    
    def call(self, inputs, is_training=False):
        x = self.conv1(inputs)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dropout1(x, is_training)
        x = self.out(x)
        
        if not is_training:
            x = tf.nn.softmax(x)
            
        return x
 

cn = ConvNet()
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

def acc(y_pred, y_true):
    y_true = tf.cast(y_true, tf.int64)
    acc_bool = tf.equal(tf.argmax(y_pred, axis=-1), y_true)
    acc_rate = tf.reduce_mean(tf.cast(acc_bool, tf.float32))
    
    return acc_rate
```
å¼€å§‹è®­ç»ƒ

```python
for step, (batch_x, batch_y) in enumerate(train_data.take(train_steps+1)):
    with tf.GradientTape() as tape:
        y_pred = cn(batch_x, is_training=True)
        batch_y = tf.cast(batch_y, tf.int32)
        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_pred, labels=batch_y))
    grads = tape.gradient(loss, cn.variables)
    optimizer.apply_gradients(grads_and_vars=zip(grads, cn.variables))
    
    if step % display_step == 0:
        
        print("step %d loss: %f  acc: %f" %(step, loss, acc(y_pred, batch_y)))
```

step 0 loss: 2.336060  acc: 0.062500  
step 100 loss: 0.187474  acc: 0.906250  
step 200 loss: 0.226720  acc: 0.968750  
step 300 loss: 0.088488  acc: 1.000000  
step 400 loss: 0.062447  acc: 0.968750  
step 500 loss: 0.162202  acc: 0.937500  
step 600 loss: 0.115830  acc: 0.968750  
step 700 loss: 0.043087  acc: 1.000000  
step 800 loss: 0.238977  acc: 0.937500  
step 900 loss: 0.008179  acc: 1.000000  
step 1000 loss: 0.007713  acc: 1.000000  
step 1100 loss: 0.085170  acc: 0.968750  
step 1200 loss: 0.065700  acc: 0.968750  
step 1300 loss: 0.056616  acc: 0.968750  
step 1400 loss: 0.018644  acc: 1.000000  
step 1500 loss: 0.029943  acc: 0.968750  
step 1600 loss: 0.103587  acc: 0.968750  
step 1700 loss: 0.005319  acc: 1.000000  
step 1800 loss: 0.004890  acc: 1.000000  
step 1900 loss: 0.000941  acc: 1.000000  
step 2000 loss: 0.057181  acc: 0.968750  

å¯è§è¿™ä¸ªç½‘ç»œè¿˜æ˜¯å¾ˆæ£’å¾ˆæ£’çš„ï½ ä¸è¿‡ç¬”è€…å¾ˆæ˜¯æ€€ç–‘æ˜¯ä¸æ˜¯è‡ªå·±çš„codingå‡ºäº†é—®é¢˜ï¼Œå±…ç„¶æœ‰é‚£ä¹ˆå¤š100%ä»¥åŠç›¸åŒçš„accï¼Œæ¯”å¦‚0.968750ã€‚åæ¥çœ‹äº†ä¸€ä¸‹ï¼Œè‡ªå·±çš„batchè®¾ç½®çš„å¾ˆå°ï¼Œ0.968750çš„accæ­£å¥½å°±æ˜¯31ä¸ªåˆ¤æ–­æ­£ç¡®ï¼Œ1ä¸ªerrorï¼Œå°±ä¸æ˜¯ä»€ä¹ˆé—®é¢˜äº†ï¼Œåªæ˜¯å› ä¸ºå¤§éƒ¨åˆ†éƒ½ä¼šæ­£ç¡®é¢„æµ‹ã€‚ç¬”è€…åˆè¯•äº†ä¸€ä¸‹ï¼Œç‰ºç‰²ä¸‹è‡ªå·±çš„ç”µè„‘ï¼ŒæŠŠbatchå¼€åˆ°äº†128ï¼Œaccç›¸åŒçš„æƒ…å†µå¥½äº†ä¸€ç‚¹ã€‚   

å’Œä¹‹å‰ä¸€æ ·æµ‹è¯•ä¸ä¸€ä¸‹ï¼Œtesté€åˆ°ç½‘ç»œå‰ä¹Ÿä¸èƒ½å¿˜äº†reshape    
åœ¨æµ‹è¯•é›†ä¸Šçš„accä¹Ÿå¥½é«˜  

```python
x_test = x_test.reshape([-1,28,28,1])
pred_test = cn(x_test, False)
acc_test = acc(pred_test, y_test)
print('test acc: %f' %(acc_test))
```

 test acc: 0.986300

```python
import matplotlib.pyplot as plt

n_images = 4

to_show_img = x_test[5:5+n_images]
y_test_pred = cn.predict(to_show_img)

for i in range(n_images):
    display_img = to_show_img[i]
    plt.imshow(display_img.reshape([28,28]), cmap='gray')
    plt.show()
    print('pred label: %d' %(tf.argmax(y_test_pred[i])))
    
    
    


```


![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8fhk6jmnnj306z06w742.jpg)

`pred label: 1`



![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8fhkt7bipj306z06wa9u.jpg)


`pred label: 4`



![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8fhl0osy9j306z06wa9u.jpg)


`pred label: 9`



![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8fhl75esbj306z06wa9u.jpg)


`pred label: 5`   
è¿™ä¸ª5å†™çš„æ˜¯çœŸä¸‘ã€‚ã€‚ã€‚


ä¸¤æ®µå¾ˆæ°´çš„ä»£ç ã€‚ã€‚ã€‚ã€‚ä»Šå¤©ç€å®æœ‰ç‚¹æ··ã€‚ã€‚ã€‚ã€‚ä¹Ÿæ²¡æœ‰å»è¿åŠ¨ğŸ¸ï¸   




-------
[^1]: sparse\_softmax\_cross\_entropy\_with\_logits, https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits


[^2]: clip\_by\_value, https://blog.csdn.net/uestc_c2_403/article/details/72190248

[^3]: origin code, https://github.com/aymericdamien/TensorFlow-Examples



