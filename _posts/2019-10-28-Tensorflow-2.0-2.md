---
layout:     post
title:      Tensorflow 2.0 02
subtitle:   AutoEncoder #å‰¯æ ‡é¢˜
date:       2019-10-28
author:     ASeeker
header-img: img/shiyuan2.jpg
catalog: true
tags:
    - tensorflow
    - deep learning
    
---




[^1]: tf2 dataset, https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take
[^2]: autoencoder, https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/autoencoder.ipynb



### autoencoder 
> Reference[^1] [^2] 

ç”¨tf2å®ç°ä¸€ä¸‹æœ€ç®€å•çš„autoencoder  
è¿™é‡Œåšä¸‹ç®€å•çš„è®°å½•ï¼Œå°±ä¸è®²aeçš„åŸç†äº†    

å†™çš„æ¯”è¾ƒä»“ä¿ƒï¼Œå¦‚æœ‰é”™è¯¯æˆ–è€…ä¾µæƒï¼Œè¯·ç•™è¨€æŒ‡æ­£/emailç¬”è€…  

```python
import tensorflow as tf
import numpy as np
```
ç¬”è€…çš„ğŸ’»ä¹Ÿåªèƒ½è·‘ä¸€è·‘mnistæ•°æ®é›†äº†ã€‚ã€‚ã€‚  

```python
from tensorflow.keras.datasets import mnist
(x_train, t_), (x_test, te_) = mnist.load_data()
```

ç”±äºæ˜¯æœ€ç®€å•çš„ æˆ‘ä»¬å°±ç›´æ¥å°†28*28çš„å›¾ç‰‡reshapeæˆä¸€ç»´çš„é•¿åº¦ä¸º784çš„vector ç›´æ¥å¡å…¥åˆ°denseç½‘ç»œé‡Œ  
åŒæ—¶å°†æ•°æ®å¤§å°æ§åˆ¶åœ¨0-1ä¹‹é—´  

```python
x_train, x_test = x_train.astype(np.float32).reshape([-1, 784]) / 255., x_test.astype(np.float32).reshape([-1, 784]) / 255.
```

ä¸€äº›å‚æ•°  

```python
batch_size = 32
features = 784
num_hidden1 = 128
num_hidden2 = 64
learning_rate = 1e-3
train_steps = 20000
```

ç”¨tfçš„datasetç®¡ç†æ–¹ä¾¿å¾ˆå¤š  
such as batch shuffleâ€¦â€¦
è¿™é‡Œæœ‰ä¸€ä¸ª`repeat()`ï¼Œä»Šå¤©ä¸Šåˆå­¦ä¹ äº†ä¸€ä¸‹ï¼Œå¦‚æœé‡Œé¢æ˜¯`None`ï¼Œé‚£ä¹ˆå°±ä¼šé‡å¤æ— é™æ¬¡ï¼Œè¿˜æœ‰ä¸€ä¸ª<a href="#1">ä¸‹æ–‡çš„</a>`take()`æ–¹æ³•ï¼Œå¯ä»¥é…åˆä½¿ç”¨ã€‚  
ï¼ˆä¸å¾—ä¸åæ§½ä¸€ä¸‹è¿™äº›ä¸œè¥¿ä¸å€ŸåŠ©æ¢¯å­éƒ½å¥½éš¾æŸ¥å•Š

```python
train_data = tf.data.Dataset.from_tensor_slices((x_train))
train_data = train_data.repeat().shuffle(10000).batch(batch_size).prefetch(1)
test_data = tf.data.Dataset.from_tensor_slices(x_test)
test_data = test_data.repeat().batch(batch_size).prefetch(1)
```

è¿™é‡Œè‡ªå·±æ‰‹å†™ä¸€ä¸‹å˜é‡  
æ³¨æ„ä¸€ä¸‹å› ä¸ºè¦é€åˆ°`tape`é‡Œé¢è®°å½•æ¢¯åº¦ï¼Œç”¨`Variable`åŒ…ä¸€ä¸‹  

```python
random_normal = tf.initializers.RandomNormal()

weights = {
    'encoder_h1': tf.Variable(random_normal([features, num_hidden1])),
    'encoder_h2': tf.Variable(random_normal([num_hidden1, num_hidden2])),
    'decoder_h1': tf.Variable(random_normal([num_hidden2, num_hidden1])),
    'decoder_h2': tf.Variable(random_normal([num_hidden1, features]))
}

biases = {
    'encoder_b1': tf.Variable(random_normal([num_hidden1])),
    'encoder_b2': tf.Variable(random_normal([num_hidden2])),
    'decoder_b1': tf.Variable(random_normal([num_hidden1])),
    'decoder_b2': tf.Variable(random_normal([features])),
    
}
```

autoencoderæœ€å…³é”®çš„ç»“æ„éƒ¨åˆ†äº†  
å…¶å®å¾ˆç®€å•  
å…ˆæŠŠé«˜ç»´é™åˆ°ä½ç»´ï¼Œç„¶åå†æ¢å¤åˆ°é«˜ç»´  
ä¸€ä¸ªenï¼Œä¸€ä¸ªde  

losså°±é‡‡ç”¨æœ€ç®€å•çš„å’ŒåŸå›¾åƒåšmseå³å¯ï¼Œä¸‹æ¬¡å†å†™ä¸€ä¸‹æ›´å®ç”¨ä¸€ç‚¹çš„vaeçš„ä»£ç     

```python
def encoder(x):
    layer1 = tf.nn.sigmoid(tf.add( tf.matmul(x, weights['encoder_h1']) ,biases['encoder_b1']))
    layer2 = tf.nn.sigmoid(tf.add( tf.matmul(layer1, weights['encoder_h2']) ,biases['encoder_b2'] ))
    
    return layer2

def decoder(x):
    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']) ,biases['decoder_b1']))
    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weights['decoder_h2']) ,biases['decoder_b2']))
    
    return layer2

def mse(recons, origin):
    return tf.reduce_mean(tf.square(recons-origin))

optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
```

ä¸‹é¢å¼€å§‹è®­ç»ƒ   
<a name="1">`take()`</a>æ–¹æ³•å°±æ˜¯å°±æ˜¯ä»datasetå–å¤šå°‘æ‰¹æ•°æ®   
å¦‚æœ`take(-1)`é‚£ä¹ˆå°±æ˜¯å»é™¤æ‰€æœ‰çš„æ•°æ®  
é‚£ä¹ˆè®¾ç½®`repeat()`ï¼Œ è¿™æ—¶å†`take(num)`ï¼Œ å°†numè®¾ç½®ä¸ºè‡ªå·±æƒ³è¦çš„æ¬¡æ•°å°±å¯ä»¥äº†ã€‚  


```python
for step, batch_x in enumerate(train_data.take(train_steps+1)):
    
    #print(batch_x.shape)
    with tf.GradientTape() as tape:
        reconstruct_x = decoder(encoder(batch_x))
        loss = mse(reconstruct_x, batch_x)
    trainable_v = list(weights.values()) + list(biases.values())
    
    grads = tape.gradient(loss, trainable_v)
    optimizer.apply_gradients(grads_and_vars=zip(grads, trainable_v))
    
    if step % 1000 == 0:
        print('step %d loss: %f' %(step, loss.numpy()))
    
```

æ¯1kæ­¥æ‰“å°ä¸€ä¸‹loss

```
step 0 loss: 0.237771
step 1000 loss: 0.058389
step 2000 loss: 0.044816
step 3000 loss: 0.037741
step 4000 loss: 0.030860
step 5000 loss: 0.029892
step 6000 loss: 0.023699
step 7000 loss: 0.020474
step 8000 loss: 0.019911
step 9000 loss: 0.013075
step 10000 loss: 0.017382
step 11000 loss: 0.016923
step 12000 loss: 0.016155
step 13000 loss: 0.012536
step 14000 loss: 0.011730
step 15000 loss: 0.012818
step 16000 loss: 0.013795
step 17000 loss: 0.013336
step 18000 loss: 0.011487
step 19000 loss: 0.011111
step 20000 loss: 0.010630
```

### æµ‹è¯•

è¿™é‡Œæµ‹è¯•ä¸€ä¸‹é‡å»ºçš„æ•ˆæœ  

```python
from matplotlib import pyplot as plt 

n = 4

origin_x = np.empty((28*n, 28*n))
reconstr_x = np.empty((28*n, 28*n))

for i, x in enumerate(test_data.take(n)):
    recon_x = decoder(encoder(x))
    
    for j in range(n):
        origin_x[i*28:(i+1)*28, j*28:(j+1)*28] = x[j].numpy().reshape([28,28])
        reconstr_x[i*28:(i+1)*28, j*28:(j+1)*28] = recon_x[j].numpy().reshape([28,28])



print("Original Images")     
plt.figure(figsize=(n, n))
plt.imshow(origin_x, origin="upper", cmap="gray")
plt.show()

print("Reconstructed Images")
plt.figure(figsize=(n, n))
plt.imshow(reconstr_x, origin="upper", cmap="gray")
plt.show()
```


åŸå§‹å›¾åƒ

![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8e4phbh1wj307506zt8l.jpg)


é‡å»ºçš„å›¾åƒ

è¿™æ˜¯åªæœ‰2kä¸ªstepé‡å»ºçš„å›¾åƒ  

![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8e4rmr0a1j30eu0dut8y.jpg)

å½“æ‹‰åˆ°20kæ—¶æ¸…æ™°äº†è®¸å¤š  

![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8e4pt34twj307506zgll.jpg)


